\chapter[Metodologia]{Metodologia}\label{capitulo4}
\addcontentsline{toc}{chapter}{Metodologia}

A abordagem metodológica para a criação de agentes inteligentes em um jogo eletrônico, utilizando redes neurais e aprendizado por reforço profundo, com o objetivo de desenvolver agentes que possam interagir de forma dinâmica com o cenário, jogadores e outros agentes, adaptando seu comportamento com base nas experiências acumuladas, passou primeiro por um extenso período de pesquisa sobre os tópicos envolvidos, com foco em redes neurais artificiais (RNAs) e aprendizado por reforço profundo (DRL).

Como é possível perceber pelo referencial teórico apresentado anteriormente, RNAs se encaixam muito bem para o objetivo deste trabalho. A rede neural irá receber entradas, processar esses dados e então alterar o comportamento do agente inteligente de acordo. Essas entradas serão observações do estado atual do ambiente, ações realizadas pelo jogador ou outros agentes, ou consequências de ações realizadas em momentos passados.

Jogos eletrônicos frequentemente apresentam ambientes complexos, o que resulta em entradas complexas para a rede neural. O aprendizado por reforço profundo é eficaz para lidar com essas complexidades porque combina o aprendizado por reforço (\textit{reinforcement learning}, ou RL) com redes neurais profundas (\textit{deep neural networks}, ou DNN), permitindo que os agentes aprendam e tomem decisões a partir de grandes quantidades de dados e identifiquem padrões complexos que seriam difíceis de modelar com técnicas tradicionais de RL \cite{mnih2015human}.

Inicialmente, será definida uma política de comportamento padrão para cada tipo de agente, a fim de dar a eles um comportamento inicial esperado. Definir uma política inicial permitirá um certo nível de controle sobre o design dos agentes, e também garantirá uma variedade de comportamentos iniciais. Esse comportamento será então alterado dinamicamente a partir de suas interações e as saídas da rede neural.

O DRL é capaz de melhorar continuamente essa política através de feedback contínuo, otimizando as ações para maximizar a recompensa acumulada ao longo do tempo. Essa capacidade de adaptação é crucial para criar agentes que se comportem de maneira adequada ao meio em que se encontram \cite{sutton2018reinforcement}.

Para as funções de ativação da rede neural a ser implementada, será utilizada a função ReLU (rectified linear unit), definida pela equação \eqref{eq:ReLU}, onde \( x \) é a entrada para o neurônio. Essa simplicidade permite cálculos rápidos e eficientes, o que é crucial para o treinamento de redes neurais profundas, onde a velocidade de processamento pode ser um fator limitante. Se a entrada \( x \) for maior ou igual a zero, a saída da ReLU é igual à entrada. Se a entrada \( x \) for menor que zero, a saída da ReLU é zero, inibindo a propagação de valores negativos \cite{krizhevsky2012imagenet}.

Em redes neurais profundas, funções de ativação como a sigmoide e a tanh podem sofrer com o problema de gradiente desvanecido, onde os gradientes se tornam muito pequenos, dificultando o aprendizado. A ReLU, por outro lado, não sofre desse problema da mesma maneira porque seu gradiente é constante (1) para valores positivos. Isso ajuda a manter gradientes significativos durante o treinamento e permite um fluxo mais eficiente de informações através das camadas da rede neural \cite{glorot2011understanding}. Essa função também promove um treinamento mais eficiente e uma melhor capacidade de generalização, pois ativa somente uma fração dos neurônios em qualquer dado ponto, pois todos os valores negativos são transformados em zero, reduzindo a redundância \cite{hinton2012layer}.

Após o processamento da rede neural, o comportamento do agente será modificado de acordo com as entradas recebidas. Como exemplo, se o agente continuamente se beneficia por ficar junto de outro agente, por exemplo, com acesso mais fácil a comida, esse agente aprenderá que sua relação com o outro é benéfica para si, e tenderá a ficar próximo desse agente. Da mesma forma, um agente carnívoro, que caça como fonte principal de comida, poderá passar a evitar certos locais onde ele sofreu muito dano do ambiente, pois sua recompensa por estar naquele local era muito baixa.

Para evitar que todos os agentes convergirem nos mesmos comportamentos, será introduzida na política padrão de comportamento de cada um um nível de tolerância a mudanças, de forma randômica, para garantir uma vasta gama de diferentes comportamentos.

O ambiente e seus agentes, assim como o jogador, serão componentes 3D, desenvolvidos com o aplicativo de modelagem Blender, uma ferramenta de código aberto com ampla quantidade de materiais educativos disponíveis online, e com uma grande quantidade de ferramentas disponíveis para utilização.

O jogo em geral, sua programação, design, mecânicas e execução da rede neural farão uso do motor gráfico Godot, também de código aberto, e que oferece ferramentas para facilitar o desenvolvimento, como renderizadores de partículas e simuladores de física, dentre outras ferramentas.

A eficácia dos agentes será avaliada com base em seu desempenho em diferentes cenários e sua capacidade de adaptação às mudanças. Métricas de desempenho incluirão a capacidade de maximizar recompensas, a adaptabilidade a diferentes estratégias do jogador, a interação com outros agentes e a variedade de comportamentos gerados. Ajustes serão feitos nas redes neurais e nos algoritmos de aprendizado por reforço para melhorar o desempenho dos agentes conforme necessário. Passados os testes iniciais, serão realizados também testes com outras pessoas, a fim de coletar feedback sobre a implementação.